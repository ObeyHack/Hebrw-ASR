{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'g6e.4xlarge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_cloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_s3_connection\n\u001b[1;32m      3\u001b[0m studio \u001b[38;5;241m=\u001b[39m Studio()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mstudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_sdk/studio.py:139\u001b[0m, in \u001b[0;36mStudio.start\u001b[0;34m(self, machine)\u001b[0m\n\u001b[1;32m    137\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m Status\u001b[38;5;241m.\u001b[39mRunning:\n\u001b[0;32m--> 139\u001b[0m     curr_machine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmachine\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_machine \u001b[38;5;241m!=\u001b[39m machine:\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    142\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested to start studio on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmachine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but studio is already running on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_machine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Consider switching instead!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_sdk/studio.py:133\u001b[0m, in \u001b[0;36mStudio.machine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m Status\u001b[38;5;241m.\u001b[39mRunning:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_studio_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_machine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_studio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_teamspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_sdk/api/studio_api.py:258\u001b[0m, in \u001b[0;36mStudioApi.get_machine\u001b[0;34m(self, studio_id, teamspace_id)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the current machine type the given Studio is running on.\"\"\"\u001b[39;00m\n\u001b[1;32m    255\u001b[0m response: V1CloudSpaceInstanceConfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mcloud_space_service_get_cloud_space_instance_config(\n\u001b[1;32m    256\u001b[0m     project_id\u001b[38;5;241m=\u001b[39mteamspace_id, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mstudio_id\n\u001b[1;32m    257\u001b[0m )\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_COMPUTE_NAME_TO_MACHINE\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_temporary_machine_patching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'g6e.4xlarge'"
     ]
    }
   ],
   "source": [
    "from lightning_sdk import Studio, Machine\n",
    "from lightning_cloud.utils import add_s3_connection\n",
    "studio = Studio()\n",
    "studio.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing KenLM dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reading package lists...\\nBuilding dependency tree...\\nReading state information...\\nlibbz2-dev is already the newest version (1.0.8-2).\\nlibboost-all-dev is already the newest version (1.71.0.0ubuntu2).\\nbuild-essential is already the newest version (12.8ubuntu1.1).\\ncmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\\nliblzma-dev is already the newest version (5.2.4-1ubuntu1.1).\\nzlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu1.5).\\n0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing KenLM toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/vchahun/kenlm.git\n",
    "!cd kenlm; ./bjam\n",
    "!python -m pip install pypi-kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/NNLP-IL/Hebrew-Resources/blob/master/corpora_and_data_resources.rst#unannotated-corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SVLM Hebrew Wikipedia Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-06 09:35:12--  https://github.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/raw/master/SVLM_Hebrew_Wikipedia_Corpus.txt\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/master/SVLM_Hebrew_Wikipedia_Corpus.txt [following]\n",
      "--2024-09-06 09:35:13--  https://raw.githubusercontent.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/master/SVLM_Hebrew_Wikipedia_Corpus.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3748102 (3.6M) [application/octet-stream]\n",
      "Saving to: ‘corpus.txt’\n",
      "\n",
      "corpus.txt          100%[===================>]   3.57M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2024-09-06 09:35:14 (195 MB/s) - ‘corpus.txt’ saved [3748102/3748102]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c \"https://github.com/NLPH/SVLM-Hebrew-Wikipedia-Corpus/raw/master/SVLM_Hebrew_Wikipedia_Corpus.txt\" -O corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset(\"HeNLP/HeDC4\",  cache_dir='datasets/text', split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Corpora used for AlephBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-06 09:35:49--  https://github.com/OnlpLab/AlephBERT/raw/main/data/wikipedia/wikipedia.raw\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/OnlpLab/AlephBERT/main/data/wikipedia/wikipedia.raw [following]\n",
      "--2024-09-06 09:35:49--  https://media.githubusercontent.com/media/OnlpLab/AlephBERT/main/data/wikipedia/wikipedia.raw\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 668508117 (638M) [image/panasonic-raw]\n",
      "Saving to: ‘wikipedia.raw’\n",
      "\n",
      "wikipedia.raw       100%[===================>] 637.54M   190MB/s    in 3.4s    \n",
      "\n",
      "2024-09-06 09:36:04 (187 MB/s) - ‘wikipedia.raw’ saved [668508117/668508117]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c \"https://github.com/OnlpLab/AlephBERT/raw/main/data/wikipedia/wikipedia.raw\" -O wikipedia.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['מָתֵמָטִיקָה היא תחום הדעת שעוסק במושגים כגון כמות, מבנה, מרחב ושינוי.\\n', 'יש המציגים אותה כמדע של דפוסים (תבניות משותפות), וכי המתמטיקאים מחפשים דפוסים: במספרים, במרחב, במדע, במחשבים ובהפשטות דמיוניות.\\n', 'המתמטיקה התפתחה ממנייה, חישוב ומדידה ומהמחקר השיטתי של צורות ותנועה של עצמים מוחשיים.\\n', 'הידע והשימוש במתמטיקה בסיסית היוו תמיד חלק טבעי וחיוני בחיי האדם והקבוצה.\\n', 'ניתן למצוא שכלולים של הרעיונות הבסיסיים בטקסטים המתמטיים שהגו המצרים, הבבלים, ההודים, הסינים, היוונים והמוסלמים.\\n', 'כבר בשלב מוקדם בלטו שלושה מאפיינים המלווים את המתמטיקה עד היום:.\\n', 'הפשטה: אף שמקורם של חלק מן העצמים המתמטיים בעולם הממשי, הדיון המתמטי בהם כרוך בהפשטה ניכרת.\\n', 'המספר 5 עשוי לייצג 5 אבנים או 5 תפוחים, אך המתמטיקה עוסקת במספר כישות עצמאית, שאינה מייצגת דבר.\\n', 'המעגל מזכיר לנו חפצים מוחשיים עגולים, כגון גלגל, אך הגאומטריה עוסקת במעגל מופשט, חסר משקל וחסר נפח ומושלם בצורתו.\\n', 'הכללה: המתמטיקה בוחנת את עצמיה המופשטים בראייה רחבה, תוך חיפוש מאפיינים כלליים שלהם.\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('wikipedia.raw', 'r') as f:\n",
    "    x = f.readlines()\n",
    "    print(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3883140/3883140 [08:49<00:00, 7334.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from hebrew import Hebrew\n",
    "from processor import get_tokenizer\n",
    "\n",
    "lines = []\n",
    "tokenizer = get_tokenizer()\n",
    "vocab = set(tokenizer.get_vocab().keys())\n",
    "\n",
    "with open('corpus.txt', 'r') as f:\n",
    "    lines += f.readlines()\n",
    "\n",
    "with open('wikipedia.raw', 'r') as f:\n",
    "    lines += f.readlines()\n",
    "\n",
    "\n",
    "output = []\n",
    "for line in tqdm(lines):\n",
    "    for sentence in nltk.sent_tokenize(line):\n",
    "        sentence_lower = ' '.join(nltk.word_tokenize(sentence)).lower()\n",
    "        sentence_norm = Hebrew(sentence_lower).text_only().string\n",
    "        sentence_vocab = ''.join([token for token in sentence_norm if token in vocab])\n",
    "        output.append(sentence_vocab)\n",
    "\n",
    "with open('corpus_preprocessed.txt', 'w') as f:\n",
    "    f.write('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /teamspace/studios/this_studio/corpus_preprocessed.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:12406332 2:37079584768 3:69524226048\n",
      "Statistics:\n",
      "1 1033861 D1=0.649208 D2=1.00786 D3+=1.33312\n",
      "2 22291297 D1=0.800538 D2=1.12305 D3+=1.35649\n",
      "3 44936683 D1=0.864941 D2=1.31992 D3+=1.47733\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 1307 assuming -p 1.5\n",
      "probing 1438 assuming -r models -p 1.5\n",
      "trie     586 without quantization\n",
      "trie     338 assuming -q 8 -b 8 quantization \n",
      "trie     538 assuming -a 22 array pointer compression\n",
      "trie     290 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:12406332 2:356660752 3:898733660\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:12406332 2:356660752 3:898733660\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Chain sizes: 1:12406332 2:65711396 3:123208864\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:104247888 kB\tVmRSS:34704 kB\tRSSMax:0 kB\tuser:0\tsys:0\tCPU:0\treal:82.9141\n"
     ]
    }
   ],
   "source": [
    "!kenlm/bin/lmplz -o 3 < corpus_preprocessed.txt > model.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the model to binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!kenlm/bin/build_binary model.arpa model.bin.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-06 09:50:18--  https://github.com/NVIDIA/NeMo/raw/main/scripts/asr_language_modeling/ngram_lm/create_lexicon_from_arpa.py\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/asr_language_modeling/ngram_lm/create_lexicon_from_arpa.py [following]\n",
      "--2024-09-06 09:50:18--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/asr_language_modeling/ngram_lm/create_lexicon_from_arpa.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3463 (3.4K) [text/plain]\n",
      "Saving to: ‘create_lexicon_from_arpa.py’\n",
      "\n",
      "create_lexicon_from 100%[===================>]   3.38K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-09-06 09:50:18 (33.0 MB/s) - ‘create_lexicon_from_arpa.py’ saved [3463/3463]\n",
      "\n",
      "[NeMo I 2024-09-06 09:50:23 create_lexicon_from_arpa:62] Writing Lexicon file to: ./model.lexicon...\n"
     ]
    }
   ],
   "source": [
    "!wget -c \"https://github.com/NVIDIA/NeMo/raw/main/scripts/asr_language_modeling/ngram_lm/create_lexicon_from_arpa.py\" -O create_lexicon_from_arpa.py\n",
    "!python create_lexicon_from_arpa.py --arpa model.arpa --dst ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading to s3 backet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/teamspace/s3_connections/audio-speech-hebrew\"\n",
    "add_s3_connection(\"audio-speech-hebrew\")\n",
    "\n",
    "studio.run(\"cp model.bin.lm /teamspace/s3_connections/audio-speech-hebrew/language_model/\")\n",
    "studio.run(\"cp model.lexicon /teamspace/s3_connections/audio-speech-hebrew/language_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing the unnecessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studio.run(\"rm -rf model.arpa\")\n",
    "studio.run(\"rm -rf corpus.txt\")\n",
    "studio.run(\"rm -rf wikipedia.raw\")\n",
    "# studio.run(\"rm -rf corpus_preprocessed.txt\")\n",
    "studio.run(\"rm -rf create_lexicon_from_arpa.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.146848678588867\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "model = kenlm.LanguageModel('model.bin.lm')\n",
    "score = model.score('שלום מה הסתברות שלי')\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
